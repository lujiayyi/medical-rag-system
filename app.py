import streamlit as st
from datetime import datetime

# é¦–å…ˆè®¾ç½®é¡µé¢é…ç½®
st.set_page_config(layout="wide")

import time
import os
from datetime import datetime
import torch
import gc
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from query_enhancement import QueryEnhancer
from rag_core import rerank_documents
import asyncio
import nest_asyncio

# ç¦ç”¨è­¦å‘Šå’Œè®¾ç½®ç¯å¢ƒå˜é‡
import warnings
warnings.filterwarnings('ignore', category=RuntimeWarning)
os.environ["TOKENIZERS_PARALLELISM"] = "false"

# åˆå§‹åŒ–å¼‚æ­¥æ”¯æŒ
def setup_async():
    try:
        import sys
        if sys.platform == 'darwin':  # macOS specific handling
            import platform
            if platform.processor() == 'arm':  # M1/M2 å¤„ç†å™¨
                print("åœ¨M1/M2 Macä¸Šç¦ç”¨å¼‚æ­¥å¾ªç¯")
                return
        
        try:
            loop = asyncio.get_running_loop()
        except RuntimeError:
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
        nest_asyncio.apply(loop)
    except Exception as e:
        print(f"Warning: å¼‚æ­¥åˆå§‹åŒ–å¤±è´¥ï¼Œè¿™ä¸ä¼šå½±å“ç³»ç»Ÿçš„ä¸»è¦åŠŸèƒ½: {e}")

# åœ¨ä¸»çº¿ç¨‹ä¸­è®¾ç½®å¼‚æ­¥æ”¯æŒ
setup_async()

# ç¦ç”¨Streamlitçš„è‡ªåŠ¨é‡æ–°è¿è¡Œå’Œæ–‡ä»¶ç›‘è§†
os.environ['STREAMLIT_SERVER_ENABLE_STATIC_SERVING'] = 'false'
os.environ['STREAMLIT_SERVER_ADDRESS'] = 'localhost'
os.environ['STREAMLIT_SERVER_PORT'] = '8501'
os.environ['STREAMLIT_SERVER_HEADLESS'] = 'true'
os.environ['STREAMLIT_SERVER_RUN_ON_SAVE'] = 'false'
os.environ['STREAMLIT_WATCHER_TYPE'] = 'none'
os.environ['STREAMLIT_SERVER_FILE_WATCHER_TYPE'] = 'none'
os.environ['STREAMLIT_BROWSER_GATHER_USAGE_STATS'] = 'false'
os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'
os.environ['HF_HOME'] = './hf_cache'
os.environ['TRANSFORMERS_CACHE'] = './model_cache'

# MPS é…ç½®ä¼˜åŒ–
os.environ['PYTORCH_MPS_HIGH_WATERMARK_RATIO'] = '0.8'
os.environ['PYTORCH_MPS_LOW_WATERMARK_RATIO'] = '0.5'
os.environ['PYTORCH_MPS_ALLOCATOR_POLICY'] = 'garbage_collection_conservative'

# æ·»åŠ å†…å­˜ç®¡ç†å‡½æ•°
def manage_memory():
    gc.collect()
    if torch.backends.mps.is_available():
        try:
            torch.mps.empty_cache()
            torch.mps.synchronize()
            allocated = torch.mps.current_allocated_memory() / 1024**3
            print(f"å½“å‰ MPS å†…å­˜ä½¿ç”¨: {allocated:.2f} GB")
        except Exception as e:
            print(f"MPS å†…å­˜æ¸…ç†å¤±è´¥: {e}")
    elif torch.cuda.is_available():
        torch.cuda.empty_cache()

# æ™ºèƒ½è®¾å¤‡é€‰æ‹©
def get_device():
    if torch.backends.mps.is_available():
        print("ä½¿ç”¨ MPS è®¾å¤‡åŠ é€Ÿ")
        return torch.device('mps')
    elif torch.cuda.is_available():
        print("ä½¿ç”¨ CUDA è®¾å¤‡åŠ é€Ÿ")
        return torch.device('cuda')
    else:
        print("ä½¿ç”¨ CPU è®¾å¤‡")
        return torch.device('cpu')

DEVICE = get_device()

# å¯¼å…¥å…¶ä»–æ¨¡å—çš„å‡½æ•°å’Œé…ç½®
from config import (
    DATA_FILE, EMBEDDING_MODEL_NAME, GENERATION_MODEL_NAME, RERANKER_MODEL_NAME, TOP_K,
    MAX_ARTICLES_TO_INDEX, MILVUS_LITE_DATA_PATH, COLLECTION_NAME,
    id_to_doc_map, EMBEDDING_DIM, HYBRID_SEARCH_CONFIG
)
from data_utils import load_data
from models import load_embedding_model, load_generation_model, load_reranker_model
from milvus_utils import get_milvus_client, setup_milvus_collection, index_data_if_needed, search_similar_documents, delete_collection_if_exists
from rag_core import generate_answer, reformulate_query

# æ·»åŠ ç¼“å­˜è£…é¥°å™¨çš„æ¨¡å‹åŠ è½½å‡½æ•°
@st.cache_resource
def load_models():
    manage_memory()
    try:
        print("å¼€å§‹åŠ è½½æ¨¡å‹...")
        # åŠ è½½ embedding æ¨¡å‹
        embedding_model = load_embedding_model(EMBEDDING_MODEL_NAME)
        manage_memory()
        
        # åŠ è½½ reranker æ¨¡å‹
        reranker_model, reranker_tokenizer = load_reranker_model(
            RERANKER_MODEL_NAME,
            device=DEVICE  # ä½¿ç”¨é€‰æ‹©çš„è®¾å¤‡
        )
        manage_memory()
        
        # åŠ è½½ç”Ÿæˆæ¨¡å‹
        generation_model, tokenizer = load_generation_model(GENERATION_MODEL_NAME)
        manage_memory()
        
        print("æ‰€æœ‰æ¨¡å‹åŠ è½½å®Œæˆ")
        return embedding_model, reranker_model, reranker_tokenizer, generation_model, tokenizer
    except Exception as e:
        st.error(f"æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        return None, None, None, None, None

# åœ¨ app.py é¡¶éƒ¨çš„ç¼“å­˜è£…é¥°å™¨éƒ¨åˆ†
@st.cache_resource
def load_query_enhancer():
    try:
        return QueryEnhancer(model_name="Qwen/Qwen2.5-0.5B")  # ä½¿ç”¨ä¸ç”Ÿæˆæ¨¡å‹ç›¸åŒçš„æ¨¡å‹
    except Exception as e:
        st.error(f"åŠ è½½æŸ¥è¯¢å¢å¼ºå™¨å¤±è´¥: {e}")
        return None

# ä¿®æ”¹ç°æœ‰çš„æ¨¡å‹åŠ è½½ä»£ç 
if 'models' not in st.session_state:
    st.session_state.models = load_models()

if 'query_enhancer' not in st.session_state:
    st.session_state.query_enhancer = load_query_enhancer()

embedding_model, reranker_model, reranker_tokenizer, generation_model, tokenizer = st.session_state.models
query_enhancer = st.session_state.query_enhancer

# Streamlit UI è®¾ç½®
st.title("ğŸ“„ åŒ»ç–— RAG ç³»ç»Ÿ (Milvus Lite)")
st.markdown(f"ä½¿ç”¨ Milvus Lite, `{EMBEDDING_MODEL_NAME}`, `{GENERATION_MODEL_NAME}`, å’Œ `{RERANKER_MODEL_NAME}`ã€‚")

# --- åˆå§‹åŒ– Milvus ---
st.title("Milvus æ•°æ®åº“ç®¡ç†")

# è·å– Milvus Lite å®¢æˆ·ç«¯
milvus_client = get_milvus_client()

if milvus_client:
    col1, col2 = st.columns(2)
    
    with col1:
        if st.button("âš ï¸ é‡æ–°åˆ›å»º Collection", help="å°†åˆ é™¤ç°æœ‰ Collection å¹¶é‡æ–°åˆ›å»º"):
            with st.spinner("åˆ é™¤å¹¶é‡æ–°åˆ›å»º Collection..."):
                delete_collection_if_exists(milvus_client, COLLECTION_NAME)
                time.sleep(1)  # ç¡®ä¿åˆ é™¤å®Œæˆ
                success = setup_milvus_collection(milvus_client)
                if success:
                    st.success("âœ… Collection å·²é‡å»º")
                    time.sleep(1)
                    st.rerun()  # é‡æ–°åŠ è½½é¡µé¢
                else:
                    st.error("âŒ Collection é‡å»ºå¤±è´¥")
    
    with col2:
        if st.button("ğŸ”„ åˆ·æ–°çŠ¶æ€", help="åˆ·æ–° Collection çŠ¶æ€"):
            st.rerun()

# --- åˆå§‹åŒ–ä¸ç¼“å­˜ç»§ç»­ ---
if milvus_client:
    # æ£€æŸ¥ Collection çš„ç»´åº¦
    try:
        schema = milvus_client.describe_collection(COLLECTION_NAME)
        vec_field = next((f for f in schema["fields"] if f["name"] == "embedding"), None)
        if vec_field:
            collection_dim = int(vec_field["params"].get("dim", -1))
            st.info(f"ğŸ“Œ å½“å‰ Collection `{COLLECTION_NAME}` çš„ç»´åº¦ï¼š{collection_dim}ï¼Œä½ çš„æ¨¡å‹é…ç½®ç»´åº¦ï¼š{EMBEDDING_DIM}")

            if collection_dim != EMBEDDING_DIM:
                st.error("ç»´åº¦ä¸ä¸€è‡´ï¼Œè¯·åˆ é™¤ Collection åé‡å»ºã€‚")

    except Exception as e:
        st.warning(f"æ— æ³•è·å– Collection ç»´åº¦ä¿¡æ¯: {e}")
        
    # è®¾ç½® Milvus Collection
    collection_is_ready = setup_milvus_collection(milvus_client)

    models_loaded = embedding_model and generation_model and tokenizer and reranker_model and reranker_tokenizer

    if collection_is_ready and models_loaded:
        pubmed_data = load_data(DATA_FILE)

        if pubmed_data:
            indexing_successful = index_data_if_needed(milvus_client, pubmed_data, embedding_model)
        else:
            st.warning(f"æ— æ³•ä» {DATA_FILE} åŠ è½½æ•°æ®ã€‚è·³è¿‡ç´¢å¼•ã€‚")
            indexing_successful = False

        st.divider()

        if not indexing_successful and not id_to_doc_map:
            st.error("æ•°æ®ç´¢å¼•å¤±è´¥æˆ–ä¸å®Œæ•´ï¼Œä¸”æ²¡æœ‰æ–‡æ¡£æ˜ å°„ã€‚RAG åŠŸèƒ½å·²ç¦ç”¨ã€‚")
        else:
            if 'chat_history' not in st.session_state:
                st.session_state.chat_history = []

            # è®¾ç½®é¡µé¢å¸ƒå±€
            col1, col2 = st.columns([3, 1])
            with col1:
                query = st.text_input("è¯·æå‡ºå…³äºå·²ç´¢å¼•åŒ»ç–—æ–‡ç« çš„é—®é¢˜:", key="query_input")
            with col2:
                enable_query_enhancement = st.toggle("å¯ç”¨æŸ¥è¯¢ä¼˜åŒ–", value=True, help="å¼€å¯åä¼šå¯¹æŸ¥è¯¢è¿›è¡Œä¼˜åŒ–ï¼Œä»¥æé«˜æ£€ç´¢æ•ˆæœ")

            # è¾“å‡ºæŸ¥è¯¢å†å²
            if st.session_state.chat_history:
                st.subheader("å†å²å¯¹è¯")
                for i, qa in enumerate(st.session_state.chat_history):
                    with st.container():
                        col1, col2 = st.columns([1, 11])
                        with col1:
                            st.write("ğŸ‘¤")
                        with col2:
                            st.markdown(f"**Q{i+1}:** {qa.get('human', '')}")
                            
                        col1, col2 = st.columns([1, 11])
                        with col1:
                            st.write("ğŸ¤–")
                        with col2:
                            st.markdown(f"**A{i+1}:** {qa.get('assistant', '')}")
                            
                        # æ˜¾ç¤ºæ£€ç´¢çš„æ–‡æ¡£æ•°é‡
                        if qa.get('retrieved_docs'):
                            st.caption(f"å‚è€ƒæ–‡æ¡£æ•°: {len(qa['retrieved_docs'])}")
                        st.divider()

            if st.button("è·å–ç­”æ¡ˆ", key="submit_button") and query:
                start_time = time.time()
                with st.spinner("æœç´¢ç›¸å…³æ–‡æ¡£ä¸­..."):
                    try:
                        if enable_query_enhancement:
                            try:
                                enhanced_query = query_enhancer.enhance_query(
                                    query,
                                    st.session_state.chat_history if st.session_state.chat_history else None
                                )
                            except Exception as e:
                                st.error(f"æŸ¥è¯¢ä¼˜åŒ–å¤±è´¥: {e}")
                                enhanced_query = query
                            if enhanced_query and enhanced_query != query:
                                st.info(f"ğŸ”„ ä¼˜åŒ–åçš„æŸ¥è¯¢: {enhanced_query}")
                        else:
                            enhanced_query = query
                    except Exception as e:
                        st.warning(f"æŸ¥è¯¢ä¼˜åŒ–å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹æŸ¥è¯¢ã€‚é”™è¯¯ï¼š{str(e)}")
                        enhanced_query = query

                # åˆ›å»ºæ··åˆæ£€ç´¢å™¨å®ä¾‹
                if 'hybrid_searcher' not in st.session_state:
                    from hybrid_search import HybridSearcher
                    st.session_state.hybrid_searcher = HybridSearcher()
                    # é¢„å¤„ç†æ‰€æœ‰æ–‡æ¡£
                    all_docs = list(id_to_doc_map.values())
                    st.session_state.hybrid_searcher.preprocess_documents(all_docs)

                with st.spinner("æ­£åœ¨è¿›è¡Œæ··åˆæ£€ç´¢..."):
                    # è·å–ç¨ å¯†å‘é‡æ£€ç´¢ç»“æœ
                    retrieved_ids, distances = search_similar_documents(
                        milvus_client, 
                        enhanced_query,  # ä½¿ç”¨å¢å¼ºåçš„æŸ¥è¯¢
                        embedding_model
                    )
                    
                    # å°†ç¨ å¯†æ£€ç´¢çš„è·ç¦»è½¬æ¢ä¸ºåˆ†æ•°ï¼ˆè¶Šå¤§è¶Šå¥½ï¼‰
                    dense_scores = [-d for d in distances]  # è½¬æ¢è·ç¦»ä¸ºåˆ†æ•°
                    
                    # è¿›è¡Œæ··åˆæ£€ç´¢
                    if retrieved_ids:
                        try:
                            # ç¡®ä¿æœ‰è¶³å¤Ÿçš„æ–‡æ¡£è¿›è¡Œæ··åˆæ£€ç´¢
                            if len(retrieved_ids) >= 2:
                                retrieved_docs, hybrid_scores = st.session_state.hybrid_searcher.hybrid_search(
                                    enhanced_query,
                                    dense_scores,
                                    retrieved_ids
                                )
                                if retrieved_docs:
                                    distances = [-s for s in hybrid_scores]  # æ›´æ–°è·ç¦»åˆ†æ•°
                                else:
                                    # å¦‚æœæ··åˆæ£€ç´¢å¤±è´¥ï¼Œå›é€€åˆ°åŸå§‹æ£€ç´¢ç»“æœ
                                    retrieved_docs = [id_to_doc_map[id] for id in retrieved_ids if id in id_to_doc_map]
                            else:
                                # æ–‡æ¡£æ•°é‡å¤ªå°‘ï¼Œä½¿ç”¨åŸå§‹æ£€ç´¢ç»“æœ
                                retrieved_docs = [id_to_doc_map[id] for id in retrieved_ids if id in id_to_doc_map]
                        except Exception as e:
                            st.warning(f"æ··åˆæ£€ç´¢å¤±è´¥: {e}ï¼Œä½¿ç”¨åŸå§‹æ£€ç´¢ç»“æœ")
                            retrieved_docs = [id_to_doc_map[id] for id in retrieved_ids if id in id_to_doc_map]

                if not retrieved_ids:
                    st.warning("åœ¨æ•°æ®åº“ä¸­æ‰¾ä¸åˆ°ç›¸å…³æ–‡æ¡£ã€‚")
                else:
                    initial_docs = [id_to_doc_map[id] for id in retrieved_ids if id in id_to_doc_map]

                    retrieved_docs = rerank_documents(query, initial_docs, reranker_model, reranker_tokenizer, DEVICE)

                    if not retrieved_docs:
                        st.error("æ£€ç´¢åˆ°çš„ ID æ— æ³•æ˜ å°„åˆ°åŠ è½½çš„æ–‡æ¡£ã€‚è¯·æ£€æŸ¥æ˜ å°„é€»è¾‘ã€‚")
                    else:
                        st.subheader("æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡æ–‡æ¡£:")
                        for i, doc in enumerate(retrieved_docs):
                            dist_str = f", è·ç¦»: {distances[i]:.4f}" if distances else ""
                            with st.expander(f"æ–‡æ¡£ {i+1} (ID: {retrieved_ids[i]}{dist_str}) - {doc['title'][:60]}"):
                                st.write(f"**æ ‡é¢˜:** {doc['title']}")
                                st.write(f"**æ‘˜è¦:** {doc['abstract']}")

                        st.divider()

                        st.subheader("ç”Ÿæˆçš„ç­”æ¡ˆ:")
                        with st.spinner("æ­£åœ¨æ ¹æ®ä¸Šä¸‹æ–‡ç”Ÿæˆç­”æ¡ˆ..."):
                            answer = generate_answer(query, retrieved_docs, generation_model, tokenizer)
                            st.write(answer)

                        st.session_state.chat_history.append({
                            "human": query,
                            "assistant": answer,
                            "retrieved_docs": retrieved_docs,
                            "timestamp": datetime.now().isoformat()
                        })

                        # åé¦ˆæ”¶é›†ç•Œé¢
                        col1, col2 = st.columns(2)
                        with col1:
                            satisfaction = st.slider("è¯·å¯¹å›ç­”çš„è´¨é‡è¿›è¡Œè¯„åˆ†(1-5åˆ†):", 1, 5, 3, key=f"satisfaction_{len(st.session_state.chat_history)}")
                        with col2:
                            feedback_type = st.radio("æ‚¨å¯¹è¿™ä¸ªå›ç­”æ»¡æ„å—ï¼Ÿ", ["æ»¡æ„", "éœ€è¦æ”¹è¿›"], key=f"feedback_{len(st.session_state.chat_history)}")
                        
                        # åˆå§‹åŒ–åé¦ˆç®¡ç†å™¨
                        if 'feedback_manager' not in st.session_state:
                            from feedback_manager import FeedbackManager
                            st.session_state.feedback_manager = FeedbackManager()

                        # æ–‡æ¡£ç›¸å…³æ€§åé¦ˆ
                        with st.expander("æ–‡æ¡£ç›¸å…³æ€§åé¦ˆ"):
                            for i, doc in enumerate(retrieved_docs[:3]):
                                st.write(f"**æ–‡æ¡£ {i+1}:** {doc['title'][:60]}")
                                doc_relevance = st.slider(f"æ­¤æ–‡æ¡£ä¸é—®é¢˜çš„ç›¸å…³æ€§(1-5åˆ†):", 1, 5, 3, key=f"doc_relevance_{i}")
                                doc_notes = st.text_input("ç›¸å…³æ€§å¤‡æ³¨(å¯é€‰):", key=f"doc_notes_{i}")
                                if st.button("æäº¤æ–‡æ¡£åé¦ˆ", key=f"submit_doc_feedback_{i}"):
                                    st.session_state.feedback_manager.add_document_feedback(
                                        doc_id=str(retrieved_ids[i]),
                                        query=query,
                                        relevance_score=doc_relevance,
                                        user_notes=doc_notes
                                    )
                                    st.success(f"æ–‡æ¡£ {i+1} çš„åé¦ˆå·²ä¿å­˜")

                        if feedback_type == "éœ€è¦æ”¹è¿›":
                            user_feedback = st.text_area("è¯·è¯¦ç»†æè¿°æ‚¨çš„åé¦ˆ:", key="user_feedback")
                            col1, col2 = st.columns(2)
                            with col1:
                                if st.button("æäº¤åé¦ˆ", key="submit_feedback"):
                                    # ä¿å­˜åé¦ˆ
                                    st.session_state.feedback_manager.add_conversation_feedback(
                                        query=query,
                                        answer=answer,
                                        feedback=user_feedback,
                                        satisfaction=satisfaction,
                                        retrieved_docs=retrieved_docs,
                                        chat_history=st.session_state.chat_history,
                                        alpha=HYBRID_SEARCH_CONFIG["vector_weight"]
                                    )
                                    st.success("æ„Ÿè°¢æ‚¨çš„åé¦ˆï¼")
                            with col2:
                                if st.button("é‡æ–°ç”Ÿæˆç­”æ¡ˆ", key="refine_button"):
                                    new_query = reformulate_query(
                                        query, 
                                        user_feedback, 
                                        answer,
                                        st.session_state.chat_history[-3:]  # ä½¿ç”¨æœ€è¿‘3è½®å¯¹è¯ä½œä¸ºä¸Šä¸‹æ–‡
                                    )
                                    st.session_state.chat_history.append({
                                        "human": new_query,
                                        "assistant": None,
                                        "retrieved_docs": None,
                                        "timestamp": datetime.now().isoformat()
                                    })
                                    # åŸºäºåé¦ˆæ›´æ–°æ··åˆæ£€ç´¢æƒé‡
                                    new_vector_weight = st.session_state.feedback_manager.get_optimal_alpha()
                                    HYBRID_SEARCH_CONFIG["vector_weight"] = new_vector_weight
                                    HYBRID_SEARCH_CONFIG["bm25_weight"] = 1.0 - new_vector_weight
                                    st.experimental_rerun()
                        else:
                            if st.button("æäº¤åé¦ˆ", key="submit_positive_feedback"):
                                st.session_state.feedback_manager.add_conversation_feedback(
                                    query=query,
                                    answer=answer,
                                    feedback="æ»¡æ„",
                                    satisfaction=satisfaction,
                                    retrieved_docs=retrieved_docs,
                                    chat_history=st.session_state.chat_history,
                                    alpha=HYBRID_SEARCH_CONFIG["vector_weight"]
                                )
                                st.success("æ„Ÿè°¢æ‚¨çš„åé¦ˆï¼")

                end_time = time.time()
                st.info(f"æ€»è€—æ—¶: {end_time - start_time:.2f} ç§’")

    else:
        st.error("åŠ è½½æ¨¡å‹æˆ–è®¾ç½® Milvus Lite collection å¤±è´¥ã€‚è¯·æ£€æŸ¥æ—¥å¿—å’Œé…ç½®ã€‚")
else:
    st.error("åˆå§‹åŒ– Milvus Lite å®¢æˆ·ç«¯å¤±è´¥ã€‚è¯·æ£€æŸ¥æ—¥å¿—ã€‚")

# --- é¡µè„š/ä¿¡æ¯ä¾§è¾¹æ  ---
st.sidebar.header("ç³»ç»Ÿé…ç½®")
st.sidebar.markdown(f"**å‘é‡å­˜å‚¨:** Milvus Lite")
st.sidebar.markdown(f"**æ•°æ®è·¯å¾„:** `{MILVUS_LITE_DATA_PATH}`")
st.sidebar.markdown(f"**Collection:** `{COLLECTION_NAME}`")
st.sidebar.markdown(f"**æ•°æ®æ–‡ä»¶:** `{DATA_FILE}`")
st.sidebar.markdown(f"**åµŒå…¥æ¨¡å‹:** `{EMBEDDING_MODEL_NAME}`")
st.sidebar.markdown(f"**ç”Ÿæˆæ¨¡å‹:** `{GENERATION_MODEL_NAME}`")
st.sidebar.markdown("**é‡æ’åºæ¨¡å‹:** `BAAI/bge-reranker-v2-m3`")
st.sidebar.markdown(f"**æœ€å¤§ç´¢å¼•æ•°:** `{MAX_ARTICLES_TO_INDEX}`")
st.sidebar.markdown(f"**æ£€ç´¢ Top K:** `{TOP_K}`")

def rerank_documents(query, docs, model, tokenizer, device):
    """å¯¹æ£€ç´¢åˆ°çš„æ–‡æ¡£è¿›è¡Œé‡æ’åº"""
    if not docs:
        return []
        
    try:
        # å‡†å¤‡é‡æ’åºçš„æ–‡æœ¬å¯¹
        text_pairs = []
        for doc in docs:
            title = doc.get('title', '')
            abstract = doc.get('abstract', '')
            content = doc.get('content', '')
            
            # ç»„åˆæ–‡æ¡£å†…å®¹
            doc_text = f"æ ‡é¢˜: {title}\næ‘˜è¦: {abstract}\nå†…å®¹: {content}"
            text_pairs.append((query, doc_text))

        # æ‰¹å¤„ç†é‡æ’åº
        scores = []
        batch_size = 4  # æ ¹æ®æ˜¾å­˜å¤§å°è°ƒæ•´
        
        for i in range(0, len(text_pairs), batch_size):
            batch_pairs = text_pairs[i:i + batch_size]
            features = tokenizer(
                batch_pairs,
                padding=True,
                truncation=True,
                return_tensors='pt',
                max_length=512
            )
            
            try:
                features = {k: v.to(device) for k, v in features.items()}
                with torch.no_grad():
                    scores.extend(model(**features).logits.flatten().tolist())
            except RuntimeError:  # å¦‚æœæ˜¾å­˜ä¸è¶³,å›é€€åˆ°CPU
                model.to('cpu')
                features = {k: v.to('cpu') for k, v in features.items()}
                with torch.no_grad():
                    scores.extend(model(**features).logits.flatten().tolist())
                if str(device).startswith('cuda'):
                    model.to(device)  # æ¢å¤åˆ°GPU

        # ç»“åˆæ–‡æ¡£å’Œåˆ†æ•°
        doc_scores = list(zip(docs, scores))
        doc_scores.sort(key=lambda x: x[1], reverse=True)
        
        return [doc for doc, _ in doc_scores]
    except Exception as e:
        st.error(f"æ–‡æ¡£é‡æ’åºå¤±è´¥: {e}")
        return docs  # å‘ç”Ÿé”™è¯¯æ—¶è¿”å›åŸå§‹æ–‡æ¡£åˆ—è¡¨